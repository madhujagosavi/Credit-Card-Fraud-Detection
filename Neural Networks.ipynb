{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data set \n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Exploring the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check if there is any null values\n",
    "df.isnull().sum() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Creating Train Set, Dev Set & Train set\n",
    "\n",
    "# Converting the csv data into matrix \n",
    "columns = \"Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount\".split()\n",
    "X = pd.DataFrame.as_matrix(df,columns=columns)\n",
    "Y = df.Class\n",
    "Y = Y.values.reshape(Y.shape[0],1)\n",
    "X.shape\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.06)\n",
    "X_test, X_dev, Y_test, Y_dev = train_test_split(X_test,Y_test, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 822, 1639, 3848, 3947, 3959, 4209, 5106, 5530, 5580, 6015, 6034,\n",
       "        7580, 8103], dtype=int64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Check if there is Classification Values - 0/1 in training set and other set \n",
    "\n",
    "np.where(Y_train == 1)\n",
    "np.where(Y_test == 1)\n",
    "np.where(Y_dev == 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : 267718\n",
      "No of test Examples : 8544\n",
      "No of dev Examples : 8545\n",
      "Shape of training data : (267718, 30)\n",
      "Shape of test data : (8544, 30)\n",
      "Shape of dev data : (8545, 30)\n",
      "Shape of Y test data : (8544, 1)\n",
      "Shape of Y dev data : (8545, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Checking the shape's of the new data set as matrix \n",
    "print(\"No of training Examples : \"+str(X_train.shape[0]))  # 94% data \n",
    "print(\"No of test Examples : \"+str(X_test.shape[0]))       # 3% data\n",
    "print(\"No of dev Examples : \"+str(X_dev.shape[0]))         # 3% data\n",
    "print(\"Shape of training data : \"+str(X_train.shape))\n",
    "print(\"Shape of test data : \"+str(X_test.shape))\n",
    "print(\"Shape of dev data : \"+str(X_dev.shape))\n",
    "print(\"Shape of Y test data : \"+str(Y_test.shape))\n",
    "print(\"Shape of Y dev data : \"+str(Y_dev.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of training Examples : (30, 267718)\n",
      "No of test Examples : (1, 267718)\n",
      "No of X_dev Examples : (30, 8545)\n",
      "No of Y_dev test Examples : (1, 8545)\n",
      "No of X_test Examples : (30, 8544)\n",
      "No of Y_test Examples : (1, 8544)\n",
      "No of Sanity_test : [ 1.47577000e+05  1.51673333e-01  1.03499042e+00 -4.52396369e-01\n",
      " -5.67173290e-01]\n"
     ]
    }
   ],
   "source": [
    "#Flatten the data to so that all Features/X Variables \n",
    "X_train_flatten = X_train.reshape(X_train.shape[0],-1).T\n",
    "Y_train_flatten = Y_train.reshape(Y_train.shape[0],-1).T\n",
    "X_dev_flatten = X_dev.reshape(X_dev.shape[0],-1).T\n",
    "Y_dev_flatten = Y_dev.reshape(Y_dev.shape[0],-1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0],-1).T\n",
    "Y_test_flatten = Y_test.reshape(Y_test.shape[0],-1).T\n",
    "\n",
    "print(\"No of training Examples : \"+str(X_train_flatten.shape))  \n",
    "print(\"No of test Examples : \"+str(Y_train_flatten.shape))  \n",
    "print(\"No of X_dev Examples : \"+str(X_dev_flatten.shape))  \n",
    "print(\"No of Y_dev test Examples : \"+str(Y_dev_flatten.shape))  \n",
    "print(\"No of X_test Examples : \"+str(X_test_flatten.shape))  \n",
    "print(\"No of Y_test Examples : \"+str(Y_test_flatten.shape))\n",
    "print(\"No of Sanity_test : \"+str(X_train_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of X_train_set shape : (30, 267718)\n",
      "No of Y_train_set shape : (1, 267718)\n"
     ]
    }
   ],
   "source": [
    "# Normalize features and create final Train set \n",
    "X_train_set = preprocessing.normalize(X_train_flatten)\n",
    "Y_train_set = Y_train_flatten\n",
    "\n",
    "print(\"No of X_train_set shape : \"+str(X_train_set.shape))  \n",
    "print(\"No of Y_train_set shape : \"+str(Y_train_set.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcation to intialize weights for forward propogration \n",
    "def intialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 =[[ 5.87599105e-04 -2.85349420e-03 -8.40927810e-04  3.39307770e-03\n",
      "  -2.69772359e-03 -3.00583700e-03  2.98244370e-03 -3.03109844e-03\n",
      "   1.47511041e-02  1.19128437e-02 -3.35639832e-03  9.63783389e-03\n",
      "   1.49160997e-02 -8.26890240e-03 -1.11978621e-02  5.98542755e-03\n",
      "  -3.03056765e-04 -6.70179488e-03  7.90472115e-03  3.44478764e-05\n",
      "   2.22908593e-03 -5.67512110e-03 -5.03278639e-03  1.00164123e-02\n",
      "   6.50929119e-03 -5.17860486e-03  1.40185167e-02  4.41698017e-03\n",
      "   1.22835714e-02  1.07207047e-02]\n",
      " [-1.75023399e-02  2.08629716e-04  1.00907953e-02 -1.00187032e-02\n",
      "  -1.04346002e-02 -7.93096695e-04 -1.35148240e-02 -1.71904202e-02\n",
      "   4.26584457e-03  4.86186077e-03 -1.99204969e-02  5.06768368e-03\n",
      "   3.75944555e-03  5.68236907e-03 -6.28233760e-03 -3.04508084e-02\n",
      "   4.28233278e-03 -8.26225155e-03  1.61059530e-02  1.49650005e-02\n",
      "   1.74944045e-03 -4.41981239e-03 -5.79471158e-03 -7.28201352e-03\n",
      "  -1.68625580e-02  3.64198719e-03 -7.41854675e-03 -1.63848924e-04\n",
      "  -5.25006941e-03  2.58877551e-02]\n",
      " [ 5.57340649e-03  3.21974583e-03  7.58702782e-03  6.97338294e-03\n",
      "  -3.70930697e-03  1.00059051e-02  1.96600527e-03  7.75189158e-03\n",
      "  -1.54706812e-02 -6.70257271e-03 -1.25280659e-02 -5.00377110e-03\n",
      "   5.20396809e-03 -1.30028979e-02  8.63834013e-03  9.76480440e-04\n",
      "  -1.12387269e-02 -4.75246096e-03 -1.60848891e-03 -1.26462845e-02\n",
      "  -1.32281106e-02 -3.31225095e-03  1.91545009e-02  1.12579711e-02\n",
      "   9.93739133e-04  1.66093157e-02  1.08327348e-02  3.18773547e-03\n",
      "   5.37034518e-04 -2.99984270e-03]\n",
      " [-3.44297856e-03 -1.11403968e-02 -2.70487354e-03 -1.20315402e-02\n",
      "   1.52888628e-02 -1.04367205e-02 -9.34876788e-03  7.31363482e-03\n",
      "   2.11292236e-04 -2.42182487e-03 -4.48103515e-03  4.33542737e-03\n",
      "  -9.40534724e-03 -1.58077765e-03  6.40430251e-03  1.16821557e-03\n",
      "   1.08741806e-02 -1.34356505e-02  5.23776396e-03 -1.05124353e-02\n",
      "   9.68352634e-03  4.72175388e-03 -1.16468857e-03 -1.22707846e-02\n",
      "   9.10337908e-04 -1.23645993e-02  6.28602084e-03  7.90385166e-04\n",
      "  -6.27412784e-03 -1.26887614e-03]\n",
      " [-1.41940032e-03  2.59553934e-03 -4.16342755e-03 -7.41769350e-03\n",
      "   3.73569914e-03 -7.73073877e-03 -3.95072499e-03 -1.34719252e-02\n",
      "   6.58385084e-03  4.36897040e-03  1.09644475e-02  2.95579527e-03\n",
      "  -9.06745355e-03  3.73593167e-03 -2.06384252e-02 -7.45722941e-03\n",
      "   1.11320250e-02 -1.81987457e-02  3.28952763e-03 -1.47371993e-02\n",
      "   1.89137534e-02  1.87019136e-02 -9.03667319e-03 -2.50735954e-03\n",
      "   6.64279771e-04 -2.67173760e-03 -1.87466232e-02  6.62650523e-03\n",
      "  -5.44568848e-03 -2.09278858e-02]\n",
      " [-5.55107666e-03  1.60732486e-02  4.43002030e-03  2.62409166e-02\n",
      "  -1.36776381e-02 -9.18526942e-04 -1.04490572e-02  6.20759219e-03\n",
      "   1.05820039e-02  1.08954338e-02 -2.27070191e-03 -2.96940447e-03\n",
      "  -5.48133690e-03 -7.33498019e-03 -2.01385690e-02  3.85835143e-03\n",
      "  -1.13293871e-03 -2.99269384e-03 -4.12224791e-03 -2.76847282e-03\n",
      "   5.49198142e-03 -5.11410079e-03  2.23831562e-03  2.59127002e-03\n",
      "   4.39383151e-04 -1.23245509e-02 -2.23762422e-02 -1.63582697e-02\n",
      "   5.08117142e-03 -2.11203387e-04]\n",
      " [-3.01566663e-03 -2.05618037e-02 -3.42276335e-03 -1.01581607e-02\n",
      "   1.88652695e-03  9.32711953e-03  1.11009543e-02 -1.83178085e-02\n",
      "  -2.90252676e-03 -7.29719035e-04  1.26115778e-02 -7.10091612e-03\n",
      "  -1.49021476e-02 -1.19368785e-02 -1.12605797e-02 -7.42450458e-03\n",
      "   1.01589409e-02  4.68558145e-03 -2.00468118e-03 -8.57810227e-03\n",
      "  -4.12235836e-03  1.17257414e-03  5.95826787e-03  1.03845128e-02\n",
      "  -2.71487741e-03 -1.54511781e-03 -1.51039119e-02 -9.04049510e-03\n",
      "  -2.13738039e-03  1.08812275e-02]\n",
      " [-6.56140971e-03 -4.47140381e-03  7.25542856e-03  6.32732777e-03\n",
      "   1.56042335e-02 -7.50379628e-04 -1.28243966e-02  1.02829285e-02\n",
      "   2.35219540e-02  1.49977785e-02  1.62775813e-02 -1.35902714e-02\n",
      "  -1.16957952e-02 -4.49208390e-03 -2.71475874e-03 -1.60510485e-02\n",
      "   5.57012938e-03 -9.19760171e-04  8.47848072e-03  1.45555605e-02\n",
      "  -5.77565073e-03  2.93209109e-03 -1.56055854e-02 -4.02550168e-03\n",
      "  -4.27730439e-03 -5.81604194e-03 -5.43425428e-03 -7.76132642e-03\n",
      "  -1.18565200e-02 -2.41123294e-02]\n",
      " [ 7.19300956e-03  6.86549419e-03 -1.05544387e-02  1.88492050e-03\n",
      "   1.84741868e-03 -1.00887133e-03  6.43105509e-03  3.64102551e-03\n",
      "  -2.39773202e-02 -1.40138457e-02 -1.15515573e-02 -4.08261191e-03\n",
      "   1.17419447e-02  1.81982612e-02  1.44575512e-03 -2.03178835e-03\n",
      "   1.39685466e-02  1.43693865e-02 -7.96382669e-03 -8.72385693e-03\n",
      "   1.25788226e-02  3.69303778e-03  4.63527677e-03 -4.96083000e-04\n",
      "  -8.88887890e-03  3.50363317e-03 -1.33928064e-02 -4.23342010e-03\n",
      "  -5.58745012e-03 -1.77878725e-03]\n",
      " [-2.04911581e-02  4.35876123e-04  5.50442397e-03 -1.27687918e-02\n",
      "   4.18582428e-03  9.93162897e-03  2.93354623e-04  1.02039652e-02\n",
      "  -1.67734343e-02  1.77045202e-02 -9.57804432e-03 -1.21825203e-02\n",
      "   2.12156237e-03 -2.48650038e-03  2.32483556e-03  9.42702436e-03\n",
      "  -1.47704396e-03 -6.45292603e-03 -1.75132489e-02 -2.96688884e-03\n",
      "   1.49269890e-03  5.60869516e-03 -1.34278297e-02 -4.22114234e-03\n",
      "   1.12840309e-04 -5.75546354e-03 -1.01841468e-02 -1.01025673e-02\n",
      "   2.31201847e-02  1.20485490e-02]\n",
      " [-1.28430358e-02 -9.58794839e-04  8.98449374e-03 -5.41855540e-03\n",
      "   1.89363820e-02  1.09231694e-02  1.62748846e-02  7.59518476e-03\n",
      "  -7.33397394e-03 -1.14090004e-02 -1.54204694e-02 -8.20051030e-03\n",
      "   1.22643040e-02  6.73147830e-03  1.68468670e-02  8.58906511e-03\n",
      "   1.16858354e-02  1.06101817e-02  1.13079906e-03 -1.56340044e-02\n",
      "  -1.63486726e-02  2.45094998e-02  6.09873548e-03  1.37680952e-02\n",
      "   1.76772230e-02  3.49682825e-03 -8.07086748e-03 -2.37555160e-03\n",
      "  -1.96371242e-02  2.36444867e-03]\n",
      " [-1.21514890e-03  2.23129145e-03 -7.88601173e-03 -7.27712885e-03\n",
      "  -1.52217821e-03  7.54769036e-03  9.63971879e-03 -1.10252122e-02\n",
      "   3.49082439e-03  4.79697360e-03  5.17700513e-03 -6.54620103e-03\n",
      "  -1.41590530e-02 -2.73942337e-03  2.43575724e-03  1.58915254e-02\n",
      "   2.25788771e-03 -3.91125870e-03 -1.68609006e-02  3.13908613e-03\n",
      "   3.11816091e-03 -3.55702549e-03  5.61051328e-05 -1.56860861e-02\n",
      "  -1.26743299e-04 -8.58349910e-03  4.93273156e-03 -1.03156194e-02\n",
      "  -9.51741735e-04  5.02279707e-03]\n",
      " [ 9.90575645e-03 -3.17529821e-03  1.58642831e-03  1.52326910e-02\n",
      "  -1.32824533e-03  1.59766449e-03 -4.65255231e-03  6.40440058e-03\n",
      "   4.02990861e-03  3.55208394e-03 -1.11645953e-02  4.38177308e-03\n",
      "  -3.60881687e-03  6.34841589e-03 -9.09359299e-04 -1.00439958e-02\n",
      "   1.49644505e-03 -1.67230148e-02  3.33909710e-03  1.46367877e-02\n",
      "  -1.61186561e-02  1.38874292e-02 -1.03360048e-02  5.07834685e-03\n",
      "   4.91697244e-03 -1.52414541e-02  1.11904164e-02  2.12346744e-03\n",
      "  -2.84957532e-03 -2.57376199e-03]\n",
      " [-5.15945481e-04  7.29923015e-04 -4.14316462e-03 -6.49303320e-03\n",
      "  -4.88586309e-03  5.94744883e-03 -2.19674457e-02  3.95221540e-03\n",
      "   7.83046235e-03  1.83571810e-03 -6.75910827e-04 -6.79686499e-03\n",
      "  -9.99851418e-03 -5.63546512e-03  9.73234486e-03 -1.70623714e-02\n",
      "   8.69638955e-03  1.45372168e-02  8.70670385e-03 -7.04997430e-03\n",
      "  -2.93398660e-03 -5.56355081e-03  1.13326490e-02  4.35311020e-03\n",
      "  -3.68439095e-03 -9.42325049e-03 -6.74749391e-03  4.28269224e-03\n",
      "   2.67938270e-03  4.50872446e-03]\n",
      " [ 6.80341446e-03  1.12631472e-02 -1.59062577e-02  4.56763877e-04\n",
      "  -6.24963404e-03 -3.71236092e-03 -7.32775949e-04 -9.03885784e-03\n",
      "   7.22866282e-03 -4.40709500e-03 -1.19400418e-02 -6.36963337e-03\n",
      "  -7.83982915e-03 -3.63434498e-03 -5.24449086e-03  8.32139505e-03\n",
      "   7.55690607e-04 -1.22485546e-02 -1.81890170e-03  1.50151747e-02\n",
      "  -5.36428796e-03 -7.10233699e-03 -2.77614274e-03  6.18027360e-03\n",
      "  -1.40550628e-02 -6.69332327e-03  1.89536078e-02  1.64864501e-02\n",
      "   4.39123649e-03 -3.89747931e-03]\n",
      " [ 6.56475656e-03 -7.23538755e-03 -3.96634346e-03 -1.05191923e-03\n",
      "  -1.09508491e-03  2.34483556e-02  1.04822353e-02  3.11443109e-03\n",
      "  -5.90547035e-03  1.64483053e-03 -1.61166343e-02  7.09112851e-03\n",
      "  -2.22316841e-02 -7.23584548e-03 -1.77604248e-02  4.37450782e-03\n",
      "   2.45748918e-02  1.34604952e-02  9.75100720e-04 -8.34211453e-03\n",
      "  -2.54554185e-03 -5.20885359e-03  2.31157466e-02  1.57618301e-02\n",
      "  -4.83665944e-03  4.15287074e-03  5.21280215e-03  8.12880196e-03\n",
      "   5.78764321e-03 -7.84701759e-03]\n",
      " [ 5.64202629e-03  1.75808589e-03 -1.01379438e-02  3.91858404e-03\n",
      "   5.25709953e-03  9.49155184e-04 -6.62306882e-03 -3.74416558e-03\n",
      "   7.35170011e-03  1.78596905e-03 -8.43462277e-03 -7.94962111e-03\n",
      "  -4.77725654e-04 -1.88329347e-02 -1.85231122e-02 -5.80341552e-03\n",
      "   1.76253054e-02  9.83615546e-03 -6.61087414e-03 -5.75727180e-03\n",
      "  -4.52665615e-03  2.54758853e-03  5.18867251e-03 -2.22494349e-02\n",
      "   6.83084965e-03 -1.25879048e-02 -7.81618760e-03  3.09390968e-03\n",
      "   1.27839298e-02  2.72610207e-03]\n",
      " [ 1.43216679e-03  7.65633762e-03  1.43026037e-02 -1.24115780e-02\n",
      "   2.02907682e-03  8.88161436e-03  6.95641807e-04  4.71991725e-03\n",
      "  -1.30204458e-02  2.00323657e-02 -2.47227673e-02 -1.55860131e-02\n",
      "   1.25003039e-02 -6.10357105e-03 -6.51126105e-03  1.02992752e-02\n",
      "   5.76284839e-03  3.41448300e-03  6.05356940e-03  1.04121883e-03\n",
      "   3.33086558e-03 -1.05262550e-03 -1.66471614e-02  2.17111559e-03\n",
      "   4.74043077e-03  1.35659849e-02  9.81006002e-03 -5.00048098e-03\n",
      "   2.27458810e-02  1.73194436e-02]\n",
      " [ 6.38085220e-03  1.27374527e-02  1.00944358e-02 -4.91291547e-03\n",
      "  -5.37457768e-03  1.31193217e-02 -5.94326894e-03 -5.27362825e-03\n",
      "  -5.16513695e-03 -2.43277375e-03  1.89294828e-02  1.60172507e-03\n",
      "   4.19336608e-03 -1.14161060e-02  1.21963571e-02  2.44740999e-03\n",
      "  -1.97618813e-02  5.12221497e-03 -1.07070622e-02  1.72830584e-02\n",
      "  -1.27009967e-02 -2.28689889e-04  8.76679684e-03 -2.02008350e-02\n",
      "   7.26695467e-03  1.11752431e-02 -1.57777871e-03 -4.72017450e-03\n",
      "   3.50876121e-03 -6.88274569e-03]\n",
      " [-2.12994328e-03 -2.53924636e-03 -1.16204987e-02 -3.42056418e-03\n",
      "  -3.63424710e-03 -1.31630695e-02 -5.54151334e-03  2.67555396e-03\n",
      "  -7.33033275e-03 -4.52024930e-03 -3.72781001e-03 -8.06183907e-03\n",
      "   3.12964255e-03  4.83711235e-03 -1.12497135e-02  2.34442018e-03\n",
      "   2.15231411e-02 -1.48296947e-02  2.66162357e-03 -9.89725574e-03\n",
      "   9.27701608e-03 -1.83143520e-03 -1.47581333e-02  1.45944371e-02\n",
      "  -4.96793686e-03  7.30940205e-03 -1.77405437e-03  5.50776911e-03\n",
      "  -8.69098189e-04 -2.17437736e-04]]\n",
      "b1 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 =[[ 1.41675543e-02  6.83964829e-03  5.89627301e-03  2.19587734e-02\n",
      "  -1.04207235e-02  1.17031444e-02 -7.69904118e-03 -4.83652539e-04\n",
      "   8.39620074e-03 -1.85590567e-03 -2.12093523e-03 -2.10938552e-02\n",
      "  -1.34698598e-02  1.97169995e-02 -2.47635495e-02  5.17225528e-03\n",
      "   4.55580448e-03 -9.72116805e-03  2.90949914e-03  3.05483905e-04]\n",
      " [ 7.85767454e-03 -6.53048029e-05  2.80492307e-03  1.38579206e-02\n",
      "  -6.22767886e-03  1.10706776e-02  9.86760224e-03 -9.19536821e-03\n",
      "  -1.60623088e-03  1.90254115e-02 -5.16498818e-04 -7.17847211e-03\n",
      "   3.99816004e-03  4.97786593e-03 -4.67383766e-03  7.98204074e-03\n",
      "   1.41423280e-02  2.12205893e-02  4.39650388e-03  2.23101181e-03]\n",
      " [ 7.70124640e-03  9.81798073e-03  5.50374974e-04  1.38760349e-02\n",
      "   1.58160052e-02  1.68859604e-03  7.43350662e-03  1.38487059e-02\n",
      "   1.97671864e-02 -7.94443196e-04 -7.80452514e-03  3.82586954e-03\n",
      "  -2.00753710e-04 -8.10827084e-03 -9.05489699e-04  2.94519913e-04\n",
      "  -9.12374193e-03 -6.10480171e-03  1.34740332e-02  4.69739785e-03]\n",
      " [-1.48174038e-03 -6.22448667e-03  2.40168713e-03 -3.17500875e-03\n",
      "  -1.71232099e-02 -4.10277251e-03  2.09938047e-03  3.36797485e-03\n",
      "  -5.99407898e-03  1.28363269e-02 -6.09571964e-03  1.39177328e-02\n",
      "   1.46033283e-03 -8.13714776e-03  6.29368592e-03 -1.65853246e-02\n",
      "  -1.28051100e-03 -5.73946224e-03  1.03854834e-02  3.48340350e-03]\n",
      " [-4.12753881e-04  4.96688193e-03 -5.07968613e-03 -8.41001529e-03\n",
      "  -2.01516794e-02  5.15044993e-03  6.55796055e-03  1.46857640e-02\n",
      "   1.66634631e-02 -6.46187245e-03  7.55755561e-03 -1.10117883e-02\n",
      "  -1.43050262e-02 -3.19915841e-03  6.86669938e-03 -9.33406883e-03\n",
      "   7.37753726e-03  1.90549793e-02  1.48707244e-02  2.10861917e-02]\n",
      " [ 3.89875886e-03 -2.89656956e-02  1.02889365e-02 -4.47238871e-03\n",
      "  -5.83792291e-03 -2.95020880e-04 -7.71409960e-03  5.05134911e-03\n",
      "   2.81707114e-04  4.19818095e-03 -7.39381176e-03 -8.56355035e-04\n",
      "   7.67734971e-03  8.51974935e-03 -5.99992914e-04 -6.87803178e-03\n",
      "  -1.05975815e-02 -8.33697516e-03  2.23087693e-03 -4.41196078e-04]\n",
      " [-1.86109198e-02 -3.03989058e-03  2.78546045e-03  2.84155505e-03\n",
      "  -7.67214638e-03 -4.51419101e-03 -3.18236411e-03  2.98325481e-03\n",
      "  -1.84759179e-03 -1.93242488e-03  8.01199393e-03 -1.26373807e-02\n",
      "  -1.04492850e-02 -1.30587144e-02  2.98449395e-03  2.97726526e-03\n",
      "   5.07682887e-03 -9.68420615e-03  6.58119538e-03  1.11133645e-03]\n",
      " [ 5.74642792e-03 -4.31261815e-03 -1.90600037e-02  7.55652703e-03\n",
      "   1.23400937e-02 -2.11341038e-03 -8.68915659e-03  6.59711440e-03\n",
      "   6.28499611e-03 -9.41658406e-03  8.67428605e-03 -1.02347850e-02\n",
      "  -2.93567446e-03 -1.14294120e-02 -3.19305535e-03  8.10768586e-04\n",
      "  -3.41729676e-04  1.44736280e-02 -3.32487783e-03 -1.30522290e-02]\n",
      " [-1.05616062e-02  7.40360260e-04 -4.05878589e-04  3.90656855e-03\n",
      "  -1.56693985e-02  1.11294857e-02 -1.44517068e-02  1.37513002e-03\n",
      "   6.09207895e-03 -4.71647952e-03 -1.88333797e-02 -1.94894591e-02\n",
      "   5.19531925e-03  4.19880184e-03  5.11775912e-03 -5.63250128e-03\n",
      "  -1.13274570e-02  5.56508002e-03  6.99700269e-03 -7.03309978e-04]\n",
      " [-1.13006586e-02  4.26732062e-03  1.22146445e-02 -1.01899997e-02\n",
      "   1.91658110e-02  2.20414404e-03 -5.62138193e-04 -3.50319840e-05\n",
      "  -2.64748182e-03  5.33969559e-03 -6.00952454e-03 -3.54496478e-03\n",
      "  -1.68863415e-02 -5.48116686e-03 -5.83682024e-03 -9.49203365e-03\n",
      "  -9.96035865e-03  4.67265214e-03 -8.49506553e-03  1.24958816e-02]]\n",
      "b2 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 =[[-0.00469879 -0.00480343  0.00659449 -0.00841194  0.00467062 -0.0028397\n",
      "  -0.00801794  0.01152579 -0.00064623 -0.00069015]\n",
      " [ 0.01398785 -0.00701987  0.0040317  -0.01588442  0.00021457  0.00400442\n",
      "  -0.0097738   0.00354505  0.00651643  0.00014695]\n",
      " [-0.01568536  0.0016155  -0.00409463 -0.00567823 -0.03222409  0.00253983\n",
      "  -0.01135529 -0.01306242  0.0186405   0.00804575]\n",
      " [-0.00910388  0.03637104  0.00342937 -0.00465017 -0.00735432 -0.01132369\n",
      "   0.00238371  0.00909588  0.00539477 -0.00316081]\n",
      " [-0.00093869 -0.00614126 -0.00149211  0.0113011  -0.02055865 -0.01188744\n",
      "   0.01315975  0.0074997  -0.0097313  -0.00938808]]\n",
      "b3 =[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W4 =[[ 0.01513102  0.00767027  0.0111203  -0.00126899 -0.01114613]\n",
      " [ 0.00020586 -0.0116111   0.00630158 -0.00279125 -0.00156701]]\n",
      "b4 =[[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Testing if the function works \n",
    "parameters = intialize_parameters([30,20,10,5,2])\n",
    "print(\"W1 =\" + str(parameters[\"W1\"]))\n",
    "print(\"b1 =\" + str(parameters[\"b1\"]))\n",
    "print(\"W2 =\" + str(parameters[\"W2\"]))\n",
    "print(\"b2 =\" + str(parameters[\"b2\"]))\n",
    "print(\"W3 =\" + str(parameters[\"W3\"]))\n",
    "print(\"b3 =\" + str(parameters[\"b3\"]))\n",
    "print(\"W4 =\" + str(parameters[\"W4\"]))\n",
    "print(\"b4 =\" + str(parameters[\"b4\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create the sigmoid function \n",
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    cache = z\n",
    "    return s,cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88079708, 0.99908895]), array([2, 7]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# test sigmoid function \n",
    "sigmoid(np.array(([2,7]))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the relu function\n",
    "def relu(z):\n",
    "    \n",
    "    r = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return r,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  0, 21]), [1, -1, 21])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# testing relu function \n",
    "relu([1,-1,21]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Backward and Sigmoid Backward\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "\n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear_forward\n",
    "def linear_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#linear_activation_forward\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L layers forward propagation \n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A, cache = linear_activation_forward(A,parameters[\"W\" + str(l)],parameters[\"b\" + str(l)],activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cost function\n",
    "\n",
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_backward \n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation\n",
    "\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache,activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters \n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\"+str(l)]=parameters[\"W\" + str(l)]-learning_rate*grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+str(l)]=parameters[\"b\" + str(l)]-learning_rate*grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the size of the network \n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "layer_dims = [30,20,10,5,1] #5 Layer model with 3 hidden layers \n",
    "\n",
    "# Deep Learning network to classify frauds and normal\n",
    "def nn_model(X,Y,layer_dims,learning_rate=.0065, num_iterations=2500,print_cost=False):\n",
    "    costs = []\n",
    "    \n",
    "    #initialize parameters \n",
    "    parameters = intialize_parameters(layer_dims)\n",
    "    # for loop for iterations/epoch \n",
    "    for i in range(0,num_iterations):\n",
    "        #forward_propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        #compute cost\n",
    "        cost = cost_function(AL, Y)\n",
    "        \n",
    "        #backward_propagation \n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 267718)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_set.shape\n",
    "Y_train_set.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.555164\n",
      "Cost after iteration 200: 0.454735\n",
      "Cost after iteration 300: 0.380457\n",
      "Cost after iteration 400: 0.324398\n",
      "Cost after iteration 500: 0.281190\n",
      "Cost after iteration 600: 0.247207\n",
      "Cost after iteration 700: 0.219982\n",
      "Cost after iteration 800: 0.197801\n",
      "Cost after iteration 900: 0.179458\n",
      "Cost after iteration 1000: 0.164087\n",
      "Cost after iteration 1100: 0.151053\n",
      "Cost after iteration 1200: 0.139882\n",
      "Cost after iteration 1300: 0.130219\n",
      "Cost after iteration 1400: 0.121789\n",
      "Cost after iteration 1500: 0.114378\n",
      "Cost after iteration 1600: 0.107818\n",
      "Cost after iteration 1700: 0.101976\n",
      "Cost after iteration 1800: 0.096743\n",
      "Cost after iteration 1900: 0.092031\n",
      "Cost after iteration 2000: 0.087769\n",
      "Cost after iteration 2100: 0.083896\n",
      "Cost after iteration 2200: 0.080364\n",
      "Cost after iteration 2300: 0.077129\n",
      "Cost after iteration 2400: 0.074158\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# running a model \n",
    "parameters = nn_model(X_train_set,Y_train_set,layer_dims,learning_rate=.0065,num_iterations = 2500, print_cost = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict Function\n",
    "def predict(X, y, parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = forward_propagation(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9982743035582221\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(X_train_set, Y_train_set, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9980102996254684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred_test = predict(X_test_flatten, Y_test_flatten, parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9984786424809828\n"
     ]
    }
   ],
   "source": [
    "pred_dev = predict(X_dev_flatten, Y_dev_flatten, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
